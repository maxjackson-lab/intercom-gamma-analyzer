# Cursor Rules for Intercom Analysis Tool

## Project Context
This is a multi-agent ETL system that analyzes Intercom conversations using specialized AI agents. The system uses:
- Python async/await patterns
- Pydantic models for validation
- Custom BaseAgent pattern for all agents
- Intercom SDK for data fetching
- Gamma API for presentation generation

## Audit After Every Code Change (MANDATORY)

**ALWAYS audit your changes immediately after making them. Never skip this.**

**ğŸš¨ CRITICAL: The correct order is:**
```
1. Make code changes
2. Run Steps 1-6 below (audit checklist)
3. Run ./scripts/run_all_checks.sh (full validation)
4. ONLY THEN commit
5. ONLY THEN push
```

**DO NOT commit before running validation. DO NOT push before validation passes.**

---

### Audit Checklist (Run After EVERY Change):

**Step 1: Read Back What You Just Changed**
```bash
# Verify the change was applied correctly
read_file <file> --offset <line_range>
```
- âœ… Confirms change was written
- âœ… Catches copy/paste errors
- âœ… Verifies indentation is correct

**Step 2: Check for Linting Errors**
```bash
read_lints ["<file_you_changed>"]
```
- âœ… Finds missing imports
- âœ… Detects type errors
- âš ï¸ **Does NOT catch async/await SyntaxErrors** (see Step 3)

**Step 3: Run Syntax Check (CRITICAL for async code)**
```bash
python3 -m py_compile <file_you_changed>
```
- âœ… **Catches 'await outside async function' immediately**
- âœ… Catches ALL Python SyntaxErrors before push
- âœ… Takes 2 seconds, prevents production crashes
- ğŸš¨ **MANDATORY for ANY changes involving async/await**

**Example (Nov 12, 2025 - Learned the Hard Way):**
- Added `await` call inside non-async function
- Linting passed (doesn't catch this!)
- Pushed to production â†’ SyntaxError crash
- **If I had run `py_compile`, would have caught it in 2 seconds**

**Step 4: Verify Imports Exist**
```bash
grep "^from.*YourNewImport|^import.*YourNewImport" <file>
```
- âœ… Confirms all imports are present
- âœ… Catches missing dependencies
- âœ… Prevents ModuleNotFoundError

**Step 5: Check for Breaking Changes**
- If you added a new method, search for where it's called
- If you changed a signature, update ALL callers
- If you added imports, verify they're in requirements.txt

**Step 6: Verify Logic is Sound**
- Read the surrounding context (20 lines before/after)
- Check that new code integrates properly
- Verify async/await patterns are correct

### When to Audit:

**Audit IMMEDIATELY after:**
- âœ… Adding new methods
- âœ… Changing function signatures
- âœ… Adding async/await calls
- âœ… Importing new dependencies
- âœ… Refactoring logic
- âœ… Fixing bugs

**ğŸš¨ BEFORE COMMITTING (MANDATORY ORDER):**

**DO NOT SKIP THIS. DO NOT COMMIT UNTIL ALL STEPS PASS.**

```bash
# Step 1-6: Run audit checklist above
# â†“
# Step 7: Run FULL validation suite
./scripts/run_all_checks.sh

# Step 8: ONLY commit if ALL checks pass
git add <files>
git commit -m "message"

# Step 9: ONLY push if commit succeeded
git push origin <branch>
```

**âŒ NEVER do this:**
- Commit before running validation
- Push before validation passes
- Skip checks because "it looks right"
- Run validation after pushing (too late!)

**âœ… ALWAYS do this:**
1. Make code changes
2. Run Steps 1-6 (audit checklist)
3. Run `./scripts/run_all_checks.sh`
4. **WAIT for results**
5. If passing â†’ commit
6. If failing â†’ fix, repeat from step 2
7. After commit succeeds â†’ push

**The pre-commit hook runs P0 checks automatically, but YOU must run ALL checks manually for significant changes.**

---

### **ğŸ“‹ COMPLETE WORKFLOW (Follow This EXACTLY):**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 1. MAKE CODE CHANGES                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                  â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 2. AUDIT STEP 1: Read back changes              â”‚
â”‚    read_file <file> --offset <range>            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                  â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 3. AUDIT STEP 2: Check linting                  â”‚
â”‚    read_lints ["<file>"]                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                  â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 4. AUDIT STEP 3: Run syntax check (async!)      â”‚
â”‚    python3 -m py_compile <file>                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                  â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 5. AUDIT STEP 4: Verify imports exist           â”‚
â”‚    grep "^from.*Import" <file>                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                  â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 6. AUDIT STEP 5: Check breaking changes         â”‚
â”‚    grep for callers if needed                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                  â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 7. AUDIT STEP 6: Verify logic is sound          â”‚
â”‚    Read surrounding context (20 lines Â±)        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                  â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 8. RUN FULL VALIDATION SUITE                    â”‚
â”‚    ./scripts/run_all_checks.sh                  â”‚
â”‚    â±ï¸  Takes 15-30 seconds                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                  â†“
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚  ALL PASS?      â”‚
         â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜
              â”‚      â”‚
         YES  â”‚      â”‚ NO
              â”‚      â”‚
              â†“      â†“
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ 9. COMMIT               â”‚  FIX ISSUES
    â”‚    git commit           â”‚  GO BACK TO
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  STEP 2
               â†“
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ 10. PUSH                â”‚
    â”‚     git push            â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**â±ï¸ TIME COMMITMENT:**
- Steps 1-6: 30 seconds - 2 minutes
- Step 7 (full validation): 15-30 seconds
- **TOTAL: 1-3 minutes to prevent 30+ minute production bugs**

---

### Why This Matters:

**Example 1 (Nov 11, 2025 - Linting):**
- Added LLM methods to TopicDetectionAgent
- Forgot to import `Optional` type hint
- Audit Step 2 (linting) caught it immediately
- Fixed before moving on

**Example 2 (Nov 12, 2025 - Syntax Check):**
- Added `await` call in non-async function
- Linting passed âœ… (doesn't catch async SyntaxErrors!)
- **Skipped Step 3 (syntax check)** âŒ
- Pushed to production â†’ SyntaxError crash
- 30 minutes debugging + emergency fix
- **If I had run `python3 -m py_compile`, caught in 2 seconds**

**Example 3 (Nov 12, 2025 - Commit Order):**
- Fixed OpenAI client bug (`.chat` â†’ `.client.chat`)
- Ran syntax check âœ…
- Ran linting âœ…
- **Committed immediately** âŒ (too early!)
- **Pushed to production** âŒ (too early!)
- THEN ran full validation suite âœ… (should have been BEFORE commit!)
- Luckily all checks passed, but if they hadn't â†’ messy git history

**Without proper order:** Dirty commits, potential rollbacks, wasted time

**With proper order:** Clean commits, validation before push, professional git history

## Code Style & Patterns

### Agent Development
- All agents inherit from `BaseAgent` and implement `execute(context: AgentContext) -> AgentResult`
- Use `AgentContext` to pass data between agents
- Return `AgentResult` with confidence scores, limitations, and sources
- Always validate input/output using `validate_input()` and `validate_output()`
- Use confidence levels: HIGH (>0.8), MEDIUM (0.6-0.8), LOW (<0.6)

### Error Handling
- Use `exponential_backoff_retry()` from `src.utils.backoff` for transient failures
- Return `AgentResult` with `success=False` instead of raising exceptions when possible
- Log errors with context: `self.logger.error("error_key", agent=agent.name, error=str(e))`
- Use circuit breaker pattern for repeated failures (see `src/utils/circuit_breaker.py`)

### Async Patterns
- Always use `async def` for agent methods
- Use `asyncio.Semaphore` for concurrency control
- Use `asyncio.wait_for()` with timeouts for agent execution
- Never block the event loop - use `await` for all I/O operations

#### LLM Operational Controls: One Mechanism at a Time (CRITICAL)

**When tuning LLM performance (timeouts, concurrency, chunking, retries), change ONE mechanism at a time.**

**Why:** Multiple interacting mechanisms (timeout + semaphore + chunk_size + retries) make failures 
impossible to attribute. You won't know which change caused the 429 errors or timeouts.

**Order of adjustment:**
1. Timeout (`self.llm_timeout`) â†’ validate via sample-mode
2. Concurrency (`self.llm_semaphore`) â†’ validate
3. Chunk size (orchestration) â†’ validate
4. Retry/fallback logic â†’ validate

**After EACH change:**
```bash
python src/main.py sample-mode --count 50 --save-to-file
# Check .log for errors, measure timing, commit if good, revert if bad
```

**See DEVELOPMENT_STANDARDS.md: "LLM Operational Controls" for detailed workflow.**

#### Async/Await Safety Rules (CRITICAL)

**Rule 1: ALWAYS await async function calls**
```python
# âŒ WRONG - Returns coroutine, not result
async def process():
    result = fetch_data()  # Forgot await!

# âœ… CORRECT
async def process():
    result = await fetch_data()
```

**Rule 2: NEVER use blocking I/O in async functions**
```python
# âŒ WRONG - Blocks entire event loop
async def process():
    time.sleep(5)  # Freezes all async tasks!
    data = requests.get(url)  # Blocks!

# âœ… CORRECT - Non-blocking alternatives
async def process():
    await asyncio.sleep(5)
    async with httpx.AsyncClient() as client:
        data = await client.get(url)
```

**Rule 3: Wrap sync DB operations in executors**
```python
# âŒ WRONG - DuckDB is sync, blocks event loop
async def save():
    storage.save_snapshot(data)

# âœ… CORRECT
async def save():
    await storage.save_snapshot_async(data)
    # Or: await loop.run_in_executor(None, storage.save_snapshot, data)
```

**Rule 4: Add timeouts to prevent hangs**
```python
# âŒ RISKY
async def enrich():
    results = await asyncio.gather(*tasks)

# âœ… SAFE
async def enrich():
    results = await asyncio.wait_for(asyncio.gather(*tasks), timeout=60)
```

**Rule 5: Use semaphores for concurrency control**
```python
# âœ… Limit concurrent API calls
semaphore = asyncio.Semaphore(5)
async with semaphore:
    return await api_call()
```

### Data Validation
- Use Pydantic models for all data structures
- Validate conversation data before processing
- Check for required fields before accessing nested data
- Use `extract_conversation_text()` utility for text extraction

#### Safe Nested Field Access (CRITICAL)

**NEVER assume Intercom fields exist - they're inconsistent!**

**Known unreliable fields** (ALWAYS use defensive access):
- `custom_attributes`, `conversation_parts`, `source.author`
- `contacts.contacts[0]`, `sla_applied`, `conversation_rating`
- `ai_agent`, `statistics.*`

**Defensive Access Pattern:**
```python
# âŒ DANGEROUS - Will crash if ANY field missing
email = conv['source']['author']['email']
category = conv['custom_attributes']['Category']

# âœ… SAFE - Graceful degradation
email = conv.get('source', {}).get('author', {}).get('email')
category = conv.get('custom_attributes', {}).get('Category')
sla_data = conv.get('sla_applied') or {}
sla_name = sla_data.get('sla_name') if isinstance(sla_data, dict) else None
```

**Normalization at Boundary:**
```python
# In intercom_sdk_service.py - normalize IMMEDIATELY after SDK fetch
def _normalize_conversation(self, conv: Dict) -> Dict:
    # conversation_parts: list â†’ dict wrapper
    if 'conversation_parts' in conv:
        parts = conv['conversation_parts']
        if isinstance(parts, list):
            conv['conversation_parts'] = {'conversation_parts': parts}
    
    # Ensure consistent types for ALL risky fields
    return conv
```

**Principle:** _"Normalize at the boundary, assume safety downstream"_

### Function Parameter Safety (CRITICAL)

**When adding/modifying function parameters, update BOTH signature AND all callers.**

**Verification Steps (MANDATORY):**

1. **Search for ALL callers before committing:**
```bash
# Find every place the function is called
grep -rn "function_name(" src/ tests/
```

2. **Update function signature with default value:**
```python
# âœ… GOOD - Backward compatible
async def analyze(data: Dict, new_param: bool = True):
    ...

# âŒ BAD - Breaks existing callers
async def analyze(data: Dict, new_param: bool):
    ...
```

3. **Update ALL callers to pass new parameter:**
```python
# Before:
await analyze(data)  # âŒ Missing new_param

# After:
await analyze(data, new_param=include_it)  # âœ…
```

4. **Verify parameter is USED in function body:**
```python
def my_func(..., new_param: bool = True):
    # âœ… Actually use it!
    if new_param:
        do_something()
```

**Common Errors:**
- Parameter in signature but not in callers â†’ TypeError
- Parameter accepted but not used â†’ Dead code
- Variable name mismatch â†’ NameError

**Recent Examples:**
- Nov 10, 2025: `_analyze_sample()` called with `include_hierarchy` but didn't accept it
- Nov 4, 2025: Variable name mismatch in Fin analysis

### Output File Resilience (CRITICAL)

**ALWAYS save complete console output to downloadable file**

**Why:** SSE connections disconnect frequently. Users lose all terminal output.

**Pattern:**
```python
# In service classes:
async def run_analysis(...):
    # Enable Rich console recording
    console.record = True
    
    # Run analysis (all console.print() captured)
    result = await self._do_analysis(...)
    
    # Export complete output
    log_output = console.export_text(clear=True)
    console.record = False
    
    # Save to file
    if save_to_file:
        log_file = output_file.with_suffix('.log')
        with open(log_file, 'w', encoding='utf-8') as f:
            f.write(log_output)
        console.print(f"ğŸ“‹ Complete log saved to: {log_file}")
```

**Apply to:** sample-mode âœ…, voice-of-customer, agent-performance, agent-coaching-report

### Hallucination Prevention
- Always use "According to the analysis" or "Based on the data" for claims
- Never invent URLs, conversation links, or external references
- State limitations when data is incomplete
- Use confidence scores to qualify uncertain insights
- Implement reflection pattern for low-confidence results (<0.7)

### Error-Log-First Debugging (CRITICAL)

**When a run fails, retrieve and analyze logs BEFORE adjusting code or configuration.**

**Workflow:**
1. Retrieve full `.log` file (not just console output)
2. Identify primary error codes (400, 429, timeout, parsing errors)
3. Map error to root cause using diagnostic table
4. Make targeted fix (not random guessing)
5. Validate fix via sample-mode
6. Check new log to confirm resolution

**Common mappings:**
- 400 Bad Request â†’ Invalid schema (check for incompatible fields like `allOf`)
- 429 Rate Limit â†’ Too many concurrent requests (reduce semaphore)
- asyncio.TimeoutError â†’ Increase timeout or reduce prompt size
- JSONDecodeError â†’ LLM returned non-JSON (fix prompt)
- ValidationError â†’ Response missing required fields (check Pydantic model)

**Why logs first?**
- Console output may be truncated on SSE disconnect
- Log files (`outputs/*.log`) persist complete output via `output_manager.py`
- Contains full error tracebacks and provider error codes
- Prevents guesswork and random config changes

**See DEVELOPMENT_STANDARDS.md: "Error-Log-First Debugging Workflow" for detailed workflow and diagnostic table.**

## File Organization

### Agent Files (`src/agents/`)
- One agent per file: `{agent_name}_agent.py`
- Import from `src.agents.base_agent` for BaseAgent, AgentContext, AgentResult
- Keep agent-specific logic in the agent class
- Use `src.agents.tools` for reusable tools

### Service Files (`src/services/`)
- API clients: `{service}_client.py` (e.g., `intercom_sdk_service.py`, `gamma_client.py`)
- Business logic: `{domain}_service.py` (e.g., `chunked_fetcher.py`)
- Use async/await for all service methods
- Implement retry logic with exponential backoff

### Utilities (`src/utils/`)
- Conversation processing: `conversation_utils.py`
- Time handling: `time_utils.py`, `timezone_utils.py`
- Retry logic: `retry.py`, `backoff.py`
- Error recovery: `circuit_breaker.py`

## API Integration Patterns

### Intercom API
- Use `IntercomSDKService` for all Intercom API calls
- Handle rate limits automatically (SDK does this)
- Use `ChunkedFetcher` for large date ranges (>3 days)
- Always deduplicate conversations by ID

### Gamma API
- Use `GammaClient` for presentation generation
- Theme names automatically resolve to themeIds (v1.0 API)
- Use `GammaGenerator` wrapper for higher-level operations
- Handle polling with exponential backoff

## Testing Patterns

### 3-Tier Testing Approach (MANDATORY):

**Tier 1: Unit Tests** (Fast, Isolated)
- Mock external dependencies
- Test single functions in isolation
- Run on every commit

**Tier 2: Integration Tests** (Moderate, Realistic)
- Use test data from `test_data.py`
- Test multiple components together
- Run before merge

**Tier 3: Real Data Validation** (Slow, Ground Truth) - **REQUIRED**
```bash
# ALWAYS run before marking feature "complete"
python src/main.py sample-mode --count 50 --save-to-file

# Then validate with real data:
python scripts/validate_data_schemas.py
```

**Why Real Data Testing Matters:**
- Test data is clean and consistent
- Real data has missing fields, type variations, edge cases
- Type inconsistencies only show up with real SDK responses
- Edge cases appear in production, not mocks

**Before marking ANY feature complete:**
- [ ] Ran with real Intercom data (not just test mode)
- [ ] Checked output files for completeness
- [ ] Verified no crashes on missing/malformed fields
- [ ] Confirmed metrics look sensible
- [ ] Tested edge cases (empty results, single result, max results)

### ğŸš¨ MANDATORY: Real-Data Sample-Mode Gate for LLM Changes

**ANY changes to LLM patterns in agents MUST pass through sample-mode testing BEFORE commit.**

This is a hard requirement, not optional. The Structured Outputs failure (causing 400 errors at scale) 
could have been caught if sample-mode testing was mandatory.

**Applies to changes in:**
- `src/agents/topic_detection_agent.py` (any LLM calls)
- `src/agents/*_agent.py` (if using LLM for classification/analysis)
- Prompt engineering, response parsing, or model selection
- Timeout configurations, retry logic, or error handling for LLMs

**MANDATORY workflow for LLM changes:**

1. **Make code changes to LLM patterns**
2. **IMMEDIATELY run sample-mode with real data:**
```bash
python src/main.py sample-mode --count 50 --save-to-file
```
3. **Verify the output:**
   - Check the `.log` file for LLM errors (400, 429, timeout, parsing errors)
   - Confirm topic classifications look reasonable
   - Verify no crashes or hangs during LLM calls
4. **Only then proceed to commit**

**Why this is mandatory:**
- LLM providers reject invalid schemas at runtime (not caught by linting)
- Rate limits and timeouts only appear under real load
- Response parsing errors only show with actual LLM outputs
- Test data doesn't exercise the full LLM code path

**Pre-commit enforcement:**
- The validation script `scripts/check_llm_features_ready.py` (if it exists) will check for recent sample-mode output
- If you modify `src/agents/*_agent.py` with LLM calls, you MUST have a recent sample-mode run
- Reference implementation: Check git diff for LLM-related changes and fail if no sample-mode output exists

**Documentation:**
See `SAMPLE_MODE_GUIDE.md` for detailed sample-mode usage and interpretation.

## Common Mistakes to Avoid

### âŒ DON'T:
- Raise exceptions in agent `execute()` methods (return AgentResult with success=False)
- Block the event loop with synchronous I/O
- Invent conversation links or URLs
- Access nested dict keys without checking existence
- Use hardcoded timeouts (use configurable timeouts)
- Skip input validation
- Add parameters to functions without updating ALL callers
- Use `time.sleep()` instead of `asyncio.sleep()` in async functions
- Use blocking I/O (requests, open()) in async functions
- Assume SDK data structure is consistent (normalize at boundary!)
- Cancel background jobs when SSE disconnects
- Only test with mock data (always validate with real data)
- Create duplicate/simplified logic in sample-mode or test utilities (use production code!)

### ğŸš¨ CRITICAL: Sample-Mode Must Use Production Code

**Problem (Nov 11, 2025):**
- Sample-mode had duplicate implementations (`_analyze_topic_detection`, `_analyze_agent_attribution`)
- Over time, production evolved but sample-mode's duplicate logic stayed frozen
- Result: Sample-mode showed bugs that didn't exist in production
- Debugging became impossible - couldn't trust sample-mode results

**Rules:**
1. âŒ NEVER copy production logic into sample-mode with "simplified" versions
2. âœ… ALWAYS import and use production agents/services directly
3. âœ… Sample-mode should ONLY add display/formatting logic, not reimpl ement detection
4. âœ… If you need debug output, enhance production agent to provide it

**Example:**
```python
# âŒ WRONG - Duplicate logic
def _analyze_topic_detection(self, conversations):
    # Simplified keyword matching that differs from production
    test_keywords = {'Billing': ['billing', 'refund'], ...}
    # ... 50 lines of duplicate logic

# âœ… CORRECT - Use production agent
def _analyze_topic_detection(self, conversations):
    from src.agents.topic_detection_agent import TopicDetectionAgent
    agent = TopicDetectionAgent()
    return agent._detect_topics_for_conversation(conv)
```

### âœ… DO:
- Return AgentResult for all agent outputs
- Use async/await for all I/O
- Ground all claims in data ("According to...")
- Use safe dict access: `conv.get('field', default)`
- Use configurable timeouts from settings
- Validate inputs before processing
- Update function signature AND all callers when adding parameters
- Use `asyncio.wait_for()` with timeouts for long operations
- Normalize Intercom data at SDK boundary (intercom_sdk_service.py)
- Let background jobs continue running on SSE disconnect
- Test with real data using sample-mode before marking complete
- Save complete console output to .log files for resilience

## Prompting Guidelines for Cursor

### When asking for agent code:
- Specify the agent's purpose and input/output format
- Include example AgentContext and AgentResult structures
- Mention confidence scoring requirements
- Specify error handling approach

### When asking for fixes:
- Include the full error traceback
- Specify which agent/service is failing
- Mention if it's a transient or permanent error
- Include relevant context (date range, conversation count, etc.)

### When asking for new features:
- Describe the agent's role in the workflow
- Specify dependencies on other agents
- Mention if it needs tools or external APIs
- Include expected confidence levels

## Performance Considerations
- Use semaphores to limit concurrent agent execution
- Batch API calls when possible
- Cache theme IDs and other static data
- Use checkpointing for long-running analyses
- Implement circuit breakers for failing services

## Security & Best Practices
- Never log API keys or sensitive tokens
- Use environment variables for configuration
- Validate all user inputs
- Sanitize conversation text before logging
- Use structured logging with context

## CLI â†” Web UI â†” Railway Alignment (CRITICAL)

### MANDATORY RULE: Every Feature Needs UI

**EVERY user-facing CLI flag MUST have a UI element.**

âŒ **NEVER add a flag without UI:**
```python
# Added to CLI but forgot UI checkbox
@click.option('--new-feature', is_flag=True)
```

âœ… **ALWAYS add UI when adding CLI flag:**
1. Add flag to CLI (`src/main.py`)
2. Add to Railway validation (`deploy/railway_web.py`)
3. Add checkbox/dropdown to UI (`deploy/railway_web.py` HTML)
4. Wire up in JavaScript (`static/app.js`)

**Verification:**
```bash
python scripts/check_cli_web_alignment.py
```

**Why This Matters:**
- Users can't use features they can't see
- CLI-only features create hidden functionality
- Leads to drift between CLI and web experience

**Example (Nov 11, 2025):**
- Added `--show-agent-thinking` flag
- Immediately added UI checkbox
- Both CLI and UI users can access feature

### The 3-Layer Contract
Every CLI command has 3 implementations that MUST stay aligned:
1. **CLI** (`src/main.py`) - Source of truth, actual implementation
2. **Railway** (`deploy/railway_web.py`) - Validation layer in CANONICAL_COMMAND_MAPPINGS
3. **Frontend** (`static/app.js`) - UI that sends commands

### MANDATORY: When Adding/Changing ANY Flag

**Always update ALL 3 layers in this order:**

1. **CLI First** (`src/main.py`):
```python
@cli.command(name='your-command')
@click.option('--your-flag', type=click.Choice(['val1', 'val2']), default='val1')
def your_command(..., your_flag: str):  # â† MUST be in signature!
    # Actually USE the flag (don't just accept it)
```

2. **Railway Second** (`deploy/railway_web.py`):
```python
CANONICAL_COMMAND_MAPPINGS = {
    'your_command': {
        'allowed_flags': {
            '--your-flag': {
                'type': 'enum',  # MUST match CLI type
                'values': ['val1', 'val2'],  # MUST match CLI choices
                'default': 'val1'  # MUST match CLI default
            }
        }
    }
}
```

3. **Frontend Last** (`static/app.js`):
```javascript
if (analysisType === 'your-command') {
    const flagValue = document.getElementById('yourFlag')?.value;
    args.push('--your-flag', flagValue);
}
```

### Verification Checklist

Before committing changes to commands/flags:
- [ ] Run `python scripts/check_cli_web_alignment.py`
- [ ] Verify flag is in CLI `@click.option`
- [ ] Verify flag is in function signature
- [ ] Verify flag is in Railway `allowed_flags`
- [ ] Verify types match (enum=Choice, boolean=is_flag)
- [ ] Verify frontend sends flag conditionally (not to all commands)
- [ ] Test in web UI (no validation errors)

### Common Mistakes
- âŒ Adding flag to Railway but forgetting CLI function signature
- âŒ Frontend sending --verbose to diagnostic modes (sample-mode)
- âŒ Type mismatch: Railway says 'boolean' but CLI uses Choice
- âŒ Values mismatch: Railway says ['quick'] but CLI says ['fast']

### Reference
See `CLI_WEB_ALIGNMENT_CHECKLIST.md` for detailed guide

## SSE and Background Execution (CRITICAL)

### Task Classification Rules

**ALWAYS use background execution for:**
1. Multi-agent analysis (ANY --multi-agent flag)
2. Week+ data with Gamma generation
3. Agent performance on long periods
4. Schema dump in deep/comprehensive mode
5. ANY task expected to run >2 minutes

**CAN use SSE streaming for:**
- Quick sample-mode (50 conversations, <1 minute)
- Single-command diagnostic tools
- Canny analysis (external API, fast)

### Background Job Resilience

**NEVER cancel jobs when SSE disconnects:**
```python
# âŒ WRONG - Kills job when browser disconnects
if await request.is_disconnected():
    await command_executor.cancel_execution(execution_id)

# âœ… CORRECT - Let job continue
if await request.is_disconnected():
    logger.info(f"Client disconnected, job continues")
    await state_manager.update_execution_status(execution_id, ExecutionStatus.RUNNING)
    break  # Exit SSE but don't cancel job
```

### Keepalive Strategy

Send keepalives BEFORE and AFTER output:
```python
while True:
    try:
        output = await asyncio.wait_for(
            output_iter.__anext__(),
            timeout=15  # Keepalive interval
        )
        yield output
    except asyncio.TimeoutError:
        yield {"event": "comment", "data": "keepalive"}
        continue
```

## Pre-Commit Validation (MANDATORY)

### ğŸ¤– Automated Checks Run on EVERY Commit

**The pre-commit hook automatically runs these checks (15-30 seconds):**

**P0 - Critical (Blocks commit if failed):**
1. âœ… **CLI â†” Web â†” Railway Alignment** - Ensures flags match across all 3 layers
   - Prevents: Validation errors, broken UI
   - Example: Adding `--your-flag` to CLI but forgetting Railway validation
   
2. âœ… **Function Signature Validation** - Checks function calls match signatures
   - Prevents: TypeError at runtime
   - Example: Calling `analyze(data, new_param=True)` but function doesn't accept `new_param`
   
3. âœ… **Async/Await Pattern Validation** - Ensures proper async usage
   - Prevents: Deadlocks, blocked event loops
   - Example: Using `time.sleep()` instead of `await asyncio.sleep()` in async function
   
4. âœ… **Import/Dependency Validation** - Verifies all imports exist
   - Prevents: ModuleNotFoundError in deployment
   - Example: Importing `some_package` not in requirements.txt

5. âœ… **Pydantic Model Instantiation** - Validates required fields are provided
   - Prevents: Pydantic ValidationError at runtime
   - Example: Creating `AgentContext(...)` without required `analysis_type` field
   - **NEW** (Nov 11, 2025): Added to catch missing Pydantic required fields

**Result:** If ANY P0 check fails â†’ Commit BLOCKED with fix instructions

**P1 - Data Quality (Run manually with `./scripts/run_all_checks.sh`):**
6. âœ… **Schema Validation** - Validates Intercom data structures (requires sample data)
7. âœ… **Null Safety** - Finds unsafe nested field access patterns
8. âœ… **Pydantic Models** - Tests models with valid/invalid data
9. âœ… **Execution Policies** - Validates SSE/background execution rules
10. âœ… **Double-Counting** - Validates topic assignment uniqueness
11. âœ… **Keyword Specificity** - Checks topic keywords aren't too broad

**Latest Run Status:**
- âœ… Null Safety: 0 critical issues (all fixed!)
- âœ… CLI Alignment: All commands aligned
- âœ… Double-Counting: Nov 4 fix verified working
- âš ï¸ Function Signatures: 18 warnings (false positives from inheritance/**kwargs)
- âš ï¸ Async Patterns: 44 warnings (blocking I/O - performance, not crashes)
- âš ï¸ Imports: 35 warnings (local modules - expected)

**Manual Run:**
```bash
# Quick P0 checks only:
./scripts/quick_checks.sh

# All checks (P0 + P1):
./scripts/run_all_checks.sh

# Specific check:
python scripts/check_null_safety.py
```

**Emergency Bypass:**
```bash
git commit --no-verify -m "emergency fix"
```

**When checks fail:**
- P0 checks: Fix before committing (blocks deployment)
- P1 checks: Fix or document why skipping
- Warnings: Review but can proceed

**Available Scripts:**
- `check_cli_web_alignment.py` - âœ… CLI/Web/Railway alignment
- `check_function_signatures.py` - Function parameter validation
- `check_async_patterns.py` - Async/await safety
- `check_imports.py` - Dependency validation
- `check_null_safety.py` - Safe field access
- `validate_data_schemas.py` - Intercom data structures
- `check_double_counting.py` - Topic uniqueness
- `validate_pydantic_models.py` - Model validation
- `check_execution_policies.py` - SSE/background policies
- `validate_topic_keywords.py` - Keyword specificity
- `run_all_checks.sh` - Master runner for all checks


