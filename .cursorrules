# Cursor Rules for Intercom Analysis Tool

## Project Context
This is a multi-agent ETL system that analyzes Intercom conversations using specialized AI agents. The system uses:
- Python async/await patterns
- Pydantic models for validation
- Custom BaseAgent pattern for all agents
- Intercom SDK for data fetching
- Gamma API for presentation generation

## Code Style & Patterns

### Agent Development
- All agents inherit from `BaseAgent` and implement `execute(context: AgentContext) -> AgentResult`
- Use `AgentContext` to pass data between agents
- Return `AgentResult` with confidence scores, limitations, and sources
- Always validate input/output using `validate_input()` and `validate_output()`
- Use confidence levels: HIGH (>0.8), MEDIUM (0.6-0.8), LOW (<0.6)

### Error Handling
- Use `exponential_backoff_retry()` from `src.utils.backoff` for transient failures
- Return `AgentResult` with `success=False` instead of raising exceptions when possible
- Log errors with context: `self.logger.error("error_key", agent=agent.name, error=str(e))`
- Use circuit breaker pattern for repeated failures (see `src/utils/circuit_breaker.py`)

### Async Patterns
- Always use `async def` for agent methods
- Use `asyncio.Semaphore` for concurrency control
- Use `asyncio.wait_for()` with timeouts for agent execution
- Never block the event loop - use `await` for all I/O operations

#### Async/Await Safety Rules (CRITICAL)

**Rule 1: ALWAYS await async function calls**
```python
# ‚ùå WRONG - Returns coroutine, not result
async def process():
    result = fetch_data()  # Forgot await!

# ‚úÖ CORRECT
async def process():
    result = await fetch_data()
```

**Rule 2: NEVER use blocking I/O in async functions**
```python
# ‚ùå WRONG - Blocks entire event loop
async def process():
    time.sleep(5)  # Freezes all async tasks!
    data = requests.get(url)  # Blocks!

# ‚úÖ CORRECT - Non-blocking alternatives
async def process():
    await asyncio.sleep(5)
    async with httpx.AsyncClient() as client:
        data = await client.get(url)
```

**Rule 3: Wrap sync DB operations in executors**
```python
# ‚ùå WRONG - DuckDB is sync, blocks event loop
async def save():
    storage.save_snapshot(data)

# ‚úÖ CORRECT
async def save():
    await storage.save_snapshot_async(data)
    # Or: await loop.run_in_executor(None, storage.save_snapshot, data)
```

**Rule 4: Add timeouts to prevent hangs**
```python
# ‚ùå RISKY
async def enrich():
    results = await asyncio.gather(*tasks)

# ‚úÖ SAFE
async def enrich():
    results = await asyncio.wait_for(asyncio.gather(*tasks), timeout=60)
```

**Rule 5: Use semaphores for concurrency control**
```python
# ‚úÖ Limit concurrent API calls
semaphore = asyncio.Semaphore(5)
async with semaphore:
    return await api_call()
```

### Data Validation
- Use Pydantic models for all data structures
- Validate conversation data before processing
- Check for required fields before accessing nested data
- Use `extract_conversation_text()` utility for text extraction

#### Safe Nested Field Access (CRITICAL)

**NEVER assume Intercom fields exist - they're inconsistent!**

**Known unreliable fields** (ALWAYS use defensive access):
- `custom_attributes`, `conversation_parts`, `source.author`
- `contacts.contacts[0]`, `sla_applied`, `conversation_rating`
- `ai_agent`, `statistics.*`

**Defensive Access Pattern:**
```python
# ‚ùå DANGEROUS - Will crash if ANY field missing
email = conv['source']['author']['email']
category = conv['custom_attributes']['Category']

# ‚úÖ SAFE - Graceful degradation
email = conv.get('source', {}).get('author', {}).get('email')
category = conv.get('custom_attributes', {}).get('Category')
sla_data = conv.get('sla_applied') or {}
sla_name = sla_data.get('sla_name') if isinstance(sla_data, dict) else None
```

**Normalization at Boundary:**
```python
# In intercom_sdk_service.py - normalize IMMEDIATELY after SDK fetch
def _normalize_conversation(self, conv: Dict) -> Dict:
    # conversation_parts: list ‚Üí dict wrapper
    if 'conversation_parts' in conv:
        parts = conv['conversation_parts']
        if isinstance(parts, list):
            conv['conversation_parts'] = {'conversation_parts': parts}
    
    # Ensure consistent types for ALL risky fields
    return conv
```

**Principle:** _"Normalize at the boundary, assume safety downstream"_

### Function Parameter Safety (CRITICAL)

**When adding/modifying function parameters, update BOTH signature AND all callers.**

**Verification Steps (MANDATORY):**

1. **Search for ALL callers before committing:**
```bash
# Find every place the function is called
grep -rn "function_name(" src/ tests/
```

2. **Update function signature with default value:**
```python
# ‚úÖ GOOD - Backward compatible
async def analyze(data: Dict, new_param: bool = True):
    ...

# ‚ùå BAD - Breaks existing callers
async def analyze(data: Dict, new_param: bool):
    ...
```

3. **Update ALL callers to pass new parameter:**
```python
# Before:
await analyze(data)  # ‚ùå Missing new_param

# After:
await analyze(data, new_param=include_it)  # ‚úÖ
```

4. **Verify parameter is USED in function body:**
```python
def my_func(..., new_param: bool = True):
    # ‚úÖ Actually use it!
    if new_param:
        do_something()
```

**Common Errors:**
- Parameter in signature but not in callers ‚Üí TypeError
- Parameter accepted but not used ‚Üí Dead code
- Variable name mismatch ‚Üí NameError

**Recent Examples:**
- Nov 10, 2025: `_analyze_sample()` called with `include_hierarchy` but didn't accept it
- Nov 4, 2025: Variable name mismatch in Fin analysis

### Output File Resilience (CRITICAL)

**ALWAYS save complete console output to downloadable file**

**Why:** SSE connections disconnect frequently. Users lose all terminal output.

**Pattern:**
```python
# In service classes:
async def run_analysis(...):
    # Enable Rich console recording
    console.record = True
    
    # Run analysis (all console.print() captured)
    result = await self._do_analysis(...)
    
    # Export complete output
    log_output = console.export_text(clear=True)
    console.record = False
    
    # Save to file
    if save_to_file:
        log_file = output_file.with_suffix('.log')
        with open(log_file, 'w', encoding='utf-8') as f:
            f.write(log_output)
        console.print(f"üìã Complete log saved to: {log_file}")
```

**Apply to:** sample-mode ‚úÖ, voice-of-customer, agent-performance, agent-coaching-report

### Hallucination Prevention
- Always use "According to the analysis" or "Based on the data" for claims
- Never invent URLs, conversation links, or external references
- State limitations when data is incomplete
- Use confidence scores to qualify uncertain insights
- Implement reflection pattern for low-confidence results (<0.7)

## File Organization

### Agent Files (`src/agents/`)
- One agent per file: `{agent_name}_agent.py`
- Import from `src.agents.base_agent` for BaseAgent, AgentContext, AgentResult
- Keep agent-specific logic in the agent class
- Use `src.agents.tools` for reusable tools

### Service Files (`src/services/`)
- API clients: `{service}_client.py` (e.g., `intercom_sdk_service.py`, `gamma_client.py`)
- Business logic: `{domain}_service.py` (e.g., `chunked_fetcher.py`)
- Use async/await for all service methods
- Implement retry logic with exponential backoff

### Utilities (`src/utils/`)
- Conversation processing: `conversation_utils.py`
- Time handling: `time_utils.py`, `timezone_utils.py`
- Retry logic: `retry.py`, `backoff.py`
- Error recovery: `circuit_breaker.py`

## API Integration Patterns

### Intercom API
- Use `IntercomSDKService` for all Intercom API calls
- Handle rate limits automatically (SDK does this)
- Use `ChunkedFetcher` for large date ranges (>3 days)
- Always deduplicate conversations by ID

### Gamma API
- Use `GammaClient` for presentation generation
- Theme names automatically resolve to themeIds (v1.0 API)
- Use `GammaGenerator` wrapper for higher-level operations
- Handle polling with exponential backoff

## Testing Patterns

### 3-Tier Testing Approach (MANDATORY):

**Tier 1: Unit Tests** (Fast, Isolated)
- Mock external dependencies
- Test single functions in isolation
- Run on every commit

**Tier 2: Integration Tests** (Moderate, Realistic)
- Use test data from `test_data.py`
- Test multiple components together
- Run before merge

**Tier 3: Real Data Validation** (Slow, Ground Truth) - **REQUIRED**
```bash
# ALWAYS run before marking feature "complete"
python src/main.py sample-mode --count 50 --save-to-file

# Then validate with real data:
python scripts/validate_data_schemas.py
```

**Why Real Data Testing Matters:**
- Test data is clean and consistent
- Real data has missing fields, type variations, edge cases
- Type inconsistencies only show up with real SDK responses
- Edge cases appear in production, not mocks

**Before marking ANY feature complete:**
- [ ] Ran with real Intercom data (not just test mode)
- [ ] Checked output files for completeness
- [ ] Verified no crashes on missing/malformed fields
- [ ] Confirmed metrics look sensible
- [ ] Tested edge cases (empty results, single result, max results)

## Common Mistakes to Avoid

### ‚ùå DON'T:
- Raise exceptions in agent `execute()` methods (return AgentResult with success=False)
- Block the event loop with synchronous I/O
- Invent conversation links or URLs
- Access nested dict keys without checking existence
- Use hardcoded timeouts (use configurable timeouts)
- Skip input validation
- Add parameters to functions without updating ALL callers
- Use `time.sleep()` instead of `asyncio.sleep()` in async functions
- Use blocking I/O (requests, open()) in async functions
- Assume SDK data structure is consistent (normalize at boundary!)
- Cancel background jobs when SSE disconnects
- Only test with mock data (always validate with real data)
- Create duplicate/simplified logic in sample-mode or test utilities (use production code!)

### üö® CRITICAL: Sample-Mode Must Use Production Code

**Problem (Nov 11, 2025):**
- Sample-mode had duplicate implementations (`_analyze_topic_detection`, `_analyze_agent_attribution`)
- Over time, production evolved but sample-mode's duplicate logic stayed frozen
- Result: Sample-mode showed bugs that didn't exist in production
- Debugging became impossible - couldn't trust sample-mode results

**Rules:**
1. ‚ùå NEVER copy production logic into sample-mode with "simplified" versions
2. ‚úÖ ALWAYS import and use production agents/services directly
3. ‚úÖ Sample-mode should ONLY add display/formatting logic, not reimpl ement detection
4. ‚úÖ If you need debug output, enhance production agent to provide it

**Example:**
```python
# ‚ùå WRONG - Duplicate logic
def _analyze_topic_detection(self, conversations):
    # Simplified keyword matching that differs from production
    test_keywords = {'Billing': ['billing', 'refund'], ...}
    # ... 50 lines of duplicate logic

# ‚úÖ CORRECT - Use production agent
def _analyze_topic_detection(self, conversations):
    from src.agents.topic_detection_agent import TopicDetectionAgent
    agent = TopicDetectionAgent()
    return agent._detect_topics_for_conversation(conv)
```

### ‚úÖ DO:
- Return AgentResult for all agent outputs
- Use async/await for all I/O
- Ground all claims in data ("According to...")
- Use safe dict access: `conv.get('field', default)`
- Use configurable timeouts from settings
- Validate inputs before processing
- Update function signature AND all callers when adding parameters
- Use `asyncio.wait_for()` with timeouts for long operations
- Normalize Intercom data at SDK boundary (intercom_sdk_service.py)
- Let background jobs continue running on SSE disconnect
- Test with real data using sample-mode before marking complete
- Save complete console output to .log files for resilience

## Prompting Guidelines for Cursor

### When asking for agent code:
- Specify the agent's purpose and input/output format
- Include example AgentContext and AgentResult structures
- Mention confidence scoring requirements
- Specify error handling approach

### When asking for fixes:
- Include the full error traceback
- Specify which agent/service is failing
- Mention if it's a transient or permanent error
- Include relevant context (date range, conversation count, etc.)

### When asking for new features:
- Describe the agent's role in the workflow
- Specify dependencies on other agents
- Mention if it needs tools or external APIs
- Include expected confidence levels

## Performance Considerations
- Use semaphores to limit concurrent agent execution
- Batch API calls when possible
- Cache theme IDs and other static data
- Use checkpointing for long-running analyses
- Implement circuit breakers for failing services

## Security & Best Practices
- Never log API keys or sensitive tokens
- Use environment variables for configuration
- Validate all user inputs
- Sanitize conversation text before logging
- Use structured logging with context

## CLI ‚Üî Web UI ‚Üî Railway Alignment (CRITICAL)

### The 3-Layer Contract
Every CLI command has 3 implementations that MUST stay aligned:
1. **CLI** (`src/main.py`) - Source of truth, actual implementation
2. **Railway** (`deploy/railway_web.py`) - Validation layer in CANONICAL_COMMAND_MAPPINGS
3. **Frontend** (`static/app.js`) - UI that sends commands

### MANDATORY: When Adding/Changing ANY Flag

**Always update ALL 3 layers in this order:**

1. **CLI First** (`src/main.py`):
```python
@cli.command(name='your-command')
@click.option('--your-flag', type=click.Choice(['val1', 'val2']), default='val1')
def your_command(..., your_flag: str):  # ‚Üê MUST be in signature!
    # Actually USE the flag (don't just accept it)
```

2. **Railway Second** (`deploy/railway_web.py`):
```python
CANONICAL_COMMAND_MAPPINGS = {
    'your_command': {
        'allowed_flags': {
            '--your-flag': {
                'type': 'enum',  # MUST match CLI type
                'values': ['val1', 'val2'],  # MUST match CLI choices
                'default': 'val1'  # MUST match CLI default
            }
        }
    }
}
```

3. **Frontend Last** (`static/app.js`):
```javascript
if (analysisType === 'your-command') {
    const flagValue = document.getElementById('yourFlag')?.value;
    args.push('--your-flag', flagValue);
}
```

### Verification Checklist

Before committing changes to commands/flags:
- [ ] Run `python scripts/check_cli_web_alignment.py`
- [ ] Verify flag is in CLI `@click.option`
- [ ] Verify flag is in function signature
- [ ] Verify flag is in Railway `allowed_flags`
- [ ] Verify types match (enum=Choice, boolean=is_flag)
- [ ] Verify frontend sends flag conditionally (not to all commands)
- [ ] Test in web UI (no validation errors)

### Common Mistakes
- ‚ùå Adding flag to Railway but forgetting CLI function signature
- ‚ùå Frontend sending --verbose to diagnostic modes (sample-mode)
- ‚ùå Type mismatch: Railway says 'boolean' but CLI uses Choice
- ‚ùå Values mismatch: Railway says ['quick'] but CLI says ['fast']

### Reference
See `CLI_WEB_ALIGNMENT_CHECKLIST.md` for detailed guide

## SSE and Background Execution (CRITICAL)

### Task Classification Rules

**ALWAYS use background execution for:**
1. Multi-agent analysis (ANY --multi-agent flag)
2. Week+ data with Gamma generation
3. Agent performance on long periods
4. Schema dump in deep/comprehensive mode
5. ANY task expected to run >2 minutes

**CAN use SSE streaming for:**
- Quick sample-mode (50 conversations, <1 minute)
- Single-command diagnostic tools
- Canny analysis (external API, fast)

### Background Job Resilience

**NEVER cancel jobs when SSE disconnects:**
```python
# ‚ùå WRONG - Kills job when browser disconnects
if await request.is_disconnected():
    await command_executor.cancel_execution(execution_id)

# ‚úÖ CORRECT - Let job continue
if await request.is_disconnected():
    logger.info(f"Client disconnected, job continues")
    await state_manager.update_execution_status(execution_id, ExecutionStatus.RUNNING)
    break  # Exit SSE but don't cancel job
```

### Keepalive Strategy

Send keepalives BEFORE and AFTER output:
```python
while True:
    try:
        output = await asyncio.wait_for(
            output_iter.__anext__(),
            timeout=15  # Keepalive interval
        )
        yield output
    except asyncio.TimeoutError:
        yield {"event": "comment", "data": "keepalive"}
        continue
```

## Pre-Commit Validation (MANDATORY)

### ü§ñ Automated Checks Run on EVERY Commit

**The pre-commit hook automatically runs these checks (15-30 seconds):**

**P0 - Critical (Blocks commit if failed):**
1. ‚úÖ **CLI ‚Üî Web ‚Üî Railway Alignment** - Ensures flags match across all 3 layers
   - Prevents: Validation errors, broken UI
   - Example: Adding `--your-flag` to CLI but forgetting Railway validation
   
2. ‚úÖ **Function Signature Validation** - Checks function calls match signatures
   - Prevents: TypeError at runtime
   - Example: Calling `analyze(data, new_param=True)` but function doesn't accept `new_param`
   
3. ‚úÖ **Async/Await Pattern Validation** - Ensures proper async usage
   - Prevents: Deadlocks, blocked event loops
   - Example: Using `time.sleep()` instead of `await asyncio.sleep()` in async function
   
4. ‚úÖ **Import/Dependency Validation** - Verifies all imports exist
   - Prevents: ModuleNotFoundError in deployment
   - Example: Importing `some_package` not in requirements.txt

5. ‚úÖ **Pydantic Model Instantiation** - Validates required fields are provided
   - Prevents: Pydantic ValidationError at runtime
   - Example: Creating `AgentContext(...)` without required `analysis_type` field
   - **NEW** (Nov 11, 2025): Added to catch missing Pydantic required fields

**Result:** If ANY P0 check fails ‚Üí Commit BLOCKED with fix instructions

**P1 - Data Quality (Run manually with `./scripts/run_all_checks.sh`):**
6. ‚úÖ **Schema Validation** - Validates Intercom data structures (requires sample data)
7. ‚úÖ **Null Safety** - Finds unsafe nested field access patterns
8. ‚úÖ **Pydantic Models** - Tests models with valid/invalid data
9. ‚úÖ **Execution Policies** - Validates SSE/background execution rules
10. ‚úÖ **Double-Counting** - Validates topic assignment uniqueness
11. ‚úÖ **Keyword Specificity** - Checks topic keywords aren't too broad

**Latest Run Status:**
- ‚úÖ Null Safety: 0 critical issues (all fixed!)
- ‚úÖ CLI Alignment: All commands aligned
- ‚úÖ Double-Counting: Nov 4 fix verified working
- ‚ö†Ô∏è Function Signatures: 18 warnings (false positives from inheritance/**kwargs)
- ‚ö†Ô∏è Async Patterns: 44 warnings (blocking I/O - performance, not crashes)
- ‚ö†Ô∏è Imports: 35 warnings (local modules - expected)

**Manual Run:**
```bash
# Quick P0 checks only:
./scripts/quick_checks.sh

# All checks (P0 + P1):
./scripts/run_all_checks.sh

# Specific check:
python scripts/check_null_safety.py
```

**Emergency Bypass:**
```bash
git commit --no-verify -m "emergency fix"
```

**When checks fail:**
- P0 checks: Fix before committing (blocks deployment)
- P1 checks: Fix or document why skipping
- Warnings: Review but can proceed

**Available Scripts:**
- `check_cli_web_alignment.py` - ‚úÖ CLI/Web/Railway alignment
- `check_function_signatures.py` - Function parameter validation
- `check_async_patterns.py` - Async/await safety
- `check_imports.py` - Dependency validation
- `check_null_safety.py` - Safe field access
- `validate_data_schemas.py` - Intercom data structures
- `check_double_counting.py` - Topic uniqueness
- `validate_pydantic_models.py` - Model validation
- `check_execution_policies.py` - SSE/background policies
- `validate_topic_keywords.py` - Keyword specificity
- `run_all_checks.sh` - Master runner for all checks


