# Observability Quick Start - Find LLM Failures Fast

**Problem:** "I can't see why LLM calls are failing"  
**Solution:** Structured JSON export + analysis script (ZERO risk, additive only)

---

## ğŸš€ How It Works (Automatic!)

**When you run analysis with `--show-agent-thinking`:**

1. âœ… Every LLM call is captured (prompt, response, tokens, model)
2. âœ… Errors are logged with context (timeout? rate limit? validation?)
3. âœ… Auto-exports to `.observability.json` file
4. âœ… Analysis script shows patterns and recommendations

**No code changes needed - just enable the flag!**

---

## ğŸ“‹ Step-by-Step Usage

### **Step 1: Run Analysis with Agent Thinking Enabled**

**In Railway UI:**
- âœ… Check **"ğŸ§  Show Agent Thinking"**
- âœ… Run your analysis (sample-mode or VOC)

**In CLI:**
```bash
python src/main.py voice-of-customer --time-period week --show-agent-thinking
```

### **Step 2: Find the Observability File**

**After run completes, look for:**
```
outputs/executions/<id>/agent_thinking_*.observability.json
```

**Or check console output:**
```
ğŸ“Š Observability data exported: agent_thinking_Nov-18-2025_11-42PM.observability.json
```

### **Step 3: Analyze the Data**

```bash
python scripts/analyze_observability.py outputs/executions/.../agent_thinking_*.observability.json
```

**Output shows:**
- âœ… Success rate (how many LLM calls worked)
- âœ… Error breakdown (timeouts? rate limits? validation?)
- âœ… Agent performance (which agents failing most)
- âœ… Token usage (cost tracking)
- âœ… Recommendations (what to fix)

---

## ğŸ“Š Example Output

```
================================================================================
AGENT OBSERVABILITY ANALYSIS
================================================================================

ğŸ“Š OVERALL STATISTICS:
   Total Events: 150
   Success Rate: 95.3%
   Errors: 7
   Total Tokens: 45,230

ğŸ“‹ EVENTS BY TYPE:
   prompt: 75
   response: 68
   error: 7

âŒ ERROR ANALYSIS (7 errors):
   Error Types:
      timeout: 5
      validation: 2
   
   Errors by Agent:
      TopicDetectionAgent: 5
      SubTopicDetectionAgent: 2

   Sample Errors:
      1. [TopicDetectionAgent] timeout
         LLM call exceeded 30s timeout
      2. [TopicDetectionAgent] validation
         LLM returned invalid topic format

RECOMMENDATIONS:
âŒ ERRORS DETECTED:
   1. Review error messages above
   2. Check if errors are timeouts (increase timeout)
   3. Check if errors are rate limits (reduce concurrency)
```

---

## ğŸ” What This Tells You

### **If Success Rate < 95%:**
- **Timeouts:** Increase `llm_timeout` or reduce prompt size
- **Rate Limits:** Reduce `llm_semaphore` (fewer concurrent calls)
- **Validation Errors:** Fix prompts or response parsing

### **If Specific Agent Failing:**
- **TopicDetectionAgent:** Check fuzzy matching logic
- **SubTopicDetectionAgent:** Check LLM validation
- **Any Agent:** Review that agent's prompts/responses

### **If High Token Usage:**
- Shows cost per agent
- Identify expensive agents
- Optimize prompts to reduce tokens

---

## ğŸ¯ Next Steps After Analysis

1. **If timeouts:** Increase timeout in agent config
2. **If rate limits:** Reduce semaphore (concurrent calls)
3. **If validation:** Check prompt format or response parsing
4. **If specific agent:** Review that agent's code

**All fixes are targeted - you know EXACTLY what to fix!**

---

## ğŸ’¡ Pro Tips

**For Railway runs:**
- Files auto-save to execution directory
- Download `.observability.json` file
- Run analysis script locally
- Fix issues based on recommendations

**For debugging:**
- Compare observability files across runs
- Track success rate over time
- Identify regressions quickly

---

## ğŸš¨ About Your Current Run

**Your completed VOC run:**
- Files ARE created (in `/app/outputs/` root)
- Browser can't find them (wrong directory - fixed in next deploy!)
- Use Railway CLI to retrieve (see `RAILWAY_FILE_ACCESS.md`)

**Next run (after deploy):**
- Files will be in execution directory âœ…
- Visible in browser âœ…
- Observability JSON included âœ…


