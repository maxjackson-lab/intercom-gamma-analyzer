"""
Data Preprocessor Service for Intercom Analysis Tool.
Handles data validation, normalization, and missing data with confidence levels.
"""

import logging
import re
from datetime import datetime, timezone
from typing import List, Dict, Optional, Any, Tuple, Set
from pathlib import Path
import json

from src.config.settings import settings
from src.models.analysis_models import ConversationSchema

logger = logging.getLogger(__name__)


class DataPreprocessor:
    """
    Comprehensive data preprocessor with confidence-based missing data handling.
    
    Features:
    - Data validation and normalization
    - Missing data inference with confidence levels
    - Deduplication and outlier detection
    - Text cleaning and standardization
    - Statistical sampling for large datasets
    """
    
    def __init__(self, config: Optional[Dict[str, Any]] = None):
        """
        Initialize data preprocessor.
        
        Args:
            config: Configuration dictionary
        """
        self.config = config or {}
        self.logger = logging.getLogger(__name__)
        
        # Confidence thresholds
        self.high_confidence_threshold = 0.8
        self.medium_confidence_threshold = 0.6
        self.low_confidence_threshold = 0.4
        
        # Text cleaning patterns
        self.html_pattern = re.compile(r'<[^>]+>')
        self.url_pattern = re.compile(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+')
        self.email_pattern = re.compile(r'\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Z|a-z]{2,}\b')
        
        # Common technical terms for inference
        self.technical_keywords = {
            'cache': ['cache', 'cached', 'clearing cache', 'clear cache'],
            'browser': ['browser', 'chrome', 'firefox', 'safari', 'edge'],
            'connection': ['connection', 'internet', 'network', 'connectivity'],
            'export': ['export', 'exporting', 'download', 'csv', 'excel'],
            'api': ['api', 'endpoint', 'request', 'response', 'authentication'],
            'billing': ['billing', 'invoice', 'payment', 'subscription', 'refund'],
            'account': ['account', 'login', 'password', 'sign in', 'authentication']
        }
        
        self.logger.info("Initialized DataPreprocessor with confidence-based inference")
    
    def preprocess_conversations(
        self, 
        conversations: List[Dict[str, Any]], 
        options: Optional[Dict[str, Any]] = None
    ) -> Tuple[List[Dict[str, Any]], Dict[str, Any]]:
        """
        Preprocess a list of conversations with comprehensive validation and inference.
        
        Args:
            conversations: Raw conversation data
            options: Preprocessing options
            
        Returns:
            Tuple of (processed_conversations, preprocessing_stats)
        """
        self.logger.info(f"Starting preprocessing of {len(conversations)} conversations")
        
        options = options or {}
        stats = {
            "original_count": len(conversations),
            "processed_count": 0,
            "deduplicated_count": 0,
            "inferred_data": {},
            "validation_errors": [],
            "confidence_levels": {"high": 0, "medium": 0, "low": 0, "none": 0}
        }
        
        try:
            # Step 1: Validate and normalize
            validated_conversations = self._validate_conversations(conversations, stats)
            self.logger.info(f"Validation completed: {len(validated_conversations)} valid conversations")
            
            # Step 2: Deduplicate
            if options.get('deduplicate', True):
                deduplicated_conversations = self._deduplicate_conversations(validated_conversations, stats)
                self.logger.info(f"Deduplication completed: {len(deduplicated_conversations)} unique conversations")
            else:
                deduplicated_conversations = validated_conversations
            
            # Step 3: Infer missing data
            if options.get('infer_missing', True):
                processed_conversations = self._infer_missing_data(deduplicated_conversations, stats)
                self.logger.info(f"Missing data inference completed")
            else:
                processed_conversations = deduplicated_conversations
            
            # Step 4: Clean and standardize text
            if options.get('clean_text', True):
                processed_conversations = self._clean_conversation_text(processed_conversations, stats)
                self.logger.info(f"Text cleaning completed")
            
            # Step 5: Detect outliers
            if options.get('detect_outliers', True):
                processed_conversations = self._detect_outliers(processed_conversations, stats)
                self.logger.info(f"Outlier detection completed")
            
            # Step 6: Statistical sampling (if needed)
            if options.get('max_conversations') and len(processed_conversations) > options['max_conversations']:
                processed_conversations = self._statistical_sampling(
                    processed_conversations, 
                    options['max_conversations'], 
                    stats
                )
                self.logger.info(f"Statistical sampling completed: {len(processed_conversations)} conversations")
            
            stats["processed_count"] = len(processed_conversations)
            
            self.logger.info(f"Preprocessing completed: {stats['processed_count']} conversations processed")
            return processed_conversations, stats
            
        except Exception as e:
            self.logger.error(f"Preprocessing failed: {e}", exc_info=True)
            raise PreprocessingError(f"Failed to preprocess conversations: {e}") from e
    
    def _validate_conversations(
        self, 
        conversations: List[Dict[str, Any]], 
        stats: Dict[str, Any]
    ) -> List[Dict[str, Any]]:
        """
        Validate conversation data and normalize fields.
        
        Uses ConversationSchema for validation and adds customer_messages field.
        Drops conversations without valid ID or usable text.
        """
        validated = []
        
        for i, conv in enumerate(conversations):
            try:
                # Basic validation
                if not isinstance(conv, dict):
                    stats["validation_errors"].append(f"Conversation {i}: Not a dictionary")
                    continue
                
                if not conv.get('id'):
                    stats["validation_errors"].append(f"Conversation {i}: Missing ID")
                    continue
                
                # Extract customer messages before schema validation
                customer_messages = self._extract_customer_messages(conv)
                conv['customer_messages'] = customer_messages
                
                # Validate with ConversationSchema
                try:
                    validated_conv = ConversationSchema(**conv)
                    
                    # Check for usable text content
                    if not validated_conv.has_usable_text():
                        stats["validation_errors"].append(
                            f"Conversation {conv.get('id')}: No usable text content (no customer messages)"
                        )
                        continue
                    
                    # Convert back to dict for downstream processing
                    conv_dict = validated_conv.dict()
                    # Preserve extra fields that were in original conv
                    for key, value in conv.items():
                        if key not in conv_dict:
                            conv_dict[key] = value
                    
                    validated.append(conv_dict)
                    
                except Exception as schema_error:
                    stats["validation_errors"].append(
                        f"Conversation {conv.get('id')}: Schema validation failed - {schema_error}"
                    )
                    continue
                    
            except Exception as e:
                self.logger.warning(f"Validation error for conversation {i}: {e}")
                stats["validation_errors"].append(f"Conversation {i}: Validation error - {e}")
                continue
        
        return validated
    
    def _extract_customer_messages(self, conv: Dict[str, Any]) -> List[str]:
        """
        Extract customer messages from raw Intercom conversation data.
        
        This method extracts all messages sent by customers (users) from both
        the initial message (source) and subsequent conversation parts.
        
        Args:
            conv: Raw Intercom conversation dictionary
            
        Returns:
            List of customer message texts
        """
        customer_msgs = []
        
        # Extract from conversation_parts
        parts = conv.get('conversation_parts', {}).get('conversation_parts', [])
        for part in parts:
            author = part.get('author', {})
            if author.get('type') == 'user':  # Customer message
                body = part.get('body', '').strip()
                if body:
                    customer_msgs.append(body)
        
        # Extract from source (initial message)
        source = conv.get('source', {})
        if source.get('author', {}).get('type') == 'user':
            body = source.get('body', '').strip()
            if body:
                customer_msgs.insert(0, body)  # Add at beginning
        
        return customer_msgs
    
    def _normalize_timestamps(self, conv: Dict[str, Any]) -> Dict[str, Any]:
        """Normalize timestamp fields to datetime objects."""
        timestamp_fields = ['created_at', 'updated_at', 'closed_at', 'last_contact_at']
        
        for field in timestamp_fields:
            if field in conv and conv[field]:
                try:
                    if isinstance(conv[field], (int, float)):
                        # Unix timestamp
                        conv[field] = datetime.fromtimestamp(conv[field], tz=timezone.utc)
                    elif isinstance(conv[field], str):
                        # ISO string
                        conv[field] = datetime.fromisoformat(conv[field].replace('Z', '+00:00'))
                except Exception as e:
                    self.logger.warning(f"Failed to normalize timestamp {field}: {e}")
                    conv[field] = None
        
        return conv
    
    def _normalize_custom_attributes(self, conv: Dict[str, Any]) -> Dict[str, Any]:
        """Normalize custom attributes structure."""
        if 'custom_attributes' not in conv:
            conv['custom_attributes'] = {}
        
        # Ensure custom_attributes is a dictionary
        if not isinstance(conv['custom_attributes'], dict):
            conv['custom_attributes'] = {}
        
        return conv
    
    def _validate_required_fields(self, conv: Dict[str, Any]) -> bool:
        """Validate that required fields are present."""
        required_fields = ['id', 'created_at']
        
        for field in required_fields:
            if not conv.get(field):
                return False
        
        return True
    
    def _deduplicate_conversations(
        self, 
        conversations: List[Dict[str, Any]], 
        stats: Dict[str, Any]
    ) -> List[Dict[str, Any]]:
        """Remove duplicate conversations based on ID."""
        seen_ids = set()
        deduplicated = []
        
        for conv in conversations:
            conv_id = conv.get('id')
            if conv_id and conv_id not in seen_ids:
                seen_ids.add(conv_id)
                deduplicated.append(conv)
        
        stats["deduplicated_count"] = len(conversations) - len(deduplicated)
        return deduplicated
    
    def _infer_missing_data(
        self, 
        conversations: List[Dict[str, Any]], 
        stats: Dict[str, Any]
    ) -> List[Dict[str, Any]]:
        """Infer missing data with confidence levels."""
        inferred_stats = {
            "categories_inferred": 0,
            "topics_inferred": 0,
            "languages_inferred": 0,
            "confidence_distribution": {"high": 0, "medium": 0, "low": 0, "none": 0}
        }
        
        for conv in conversations:
            # Infer category from text content
            category, confidence = self._infer_category(conv)
            if category and confidence > self.low_confidence_threshold:
                conv['inferred_category'] = category
                conv['category_confidence'] = confidence
                inferred_stats["categories_inferred"] += 1
                inferred_stats["confidence_distribution"][self._get_confidence_level(confidence)] += 1
            
            # Infer topics from text content
            topics, confidence = self._infer_topics(conv)
            if topics and confidence > self.low_confidence_threshold:
                conv['inferred_topics'] = topics
                conv['topics_confidence'] = confidence
                inferred_stats["topics_inferred"] += 1
            
            # Infer language from text content
            language, confidence = self._infer_language(conv)
            if language and confidence > self.low_confidence_threshold:
                conv['inferred_language'] = language
                conv['language_confidence'] = confidence
                inferred_stats["languages_inferred"] += 1
        
        stats["inferred_data"] = inferred_stats
        return conversations
    
    def _infer_category(self, conv: Dict[str, Any]) -> Tuple[Optional[str], float]:
        """Infer conversation category from text content."""
        text = self._extract_conversation_text(conv).lower()
        
        # Check for technical keywords
        for category, keywords in self.technical_keywords.items():
            matches = sum(1 for keyword in keywords if keyword in text)
            if matches > 0:
                confidence = min(0.9, 0.5 + (matches * 0.1))
                return category, confidence
        
        # Check for billing keywords
        billing_keywords = ['refund', 'invoice', 'payment', 'subscription', 'billing', 'charge']
        if any(keyword in text for keyword in billing_keywords):
            return 'billing', 0.7
        
        # Check for account keywords
        account_keywords = ['login', 'password', 'account', 'sign in', 'authentication']
        if any(keyword in text for keyword in account_keywords):
            return 'account', 0.6
        
        return None, 0.0
    
    def _infer_topics(self, conv: Dict[str, Any]) -> Tuple[List[str], float]:
        """Infer conversation topics from text content."""
        text = self._extract_conversation_text(conv).lower()
        topics = []
        confidence = 0.0
        
        # Simple keyword-based topic inference
        topic_keywords = {
            'export': ['export', 'download', 'csv', 'excel', 'data export'],
            'api': ['api', 'endpoint', 'request', 'response', 'authentication'],
            'browser': ['browser', 'chrome', 'firefox', 'safari', 'edge'],
            'cache': ['cache', 'cached', 'clearing cache', 'clear cache'],
            'connection': ['connection', 'internet', 'network', 'connectivity'],
            'refund': ['refund', 'money back', 'cancel subscription'],
            'invoice': ['invoice', 'billing', 'payment', 'charge']
        }
        
        for topic, keywords in topic_keywords.items():
            matches = sum(1 for keyword in keywords if keyword in text)
            if matches > 0:
                topics.append(topic)
                confidence = max(confidence, min(0.8, 0.4 + (matches * 0.1)))
        
        return topics, confidence
    
    def _infer_language(self, conv: Dict[str, Any]) -> Tuple[Optional[str], float]:
        """Infer conversation language from text content."""
        text = self._extract_conversation_text(conv)
        
        # Simple language detection based on common words
        english_words = ['the', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'for', 'of', 'with', 'by']
        spanish_words = ['el', 'la', 'de', 'que', 'y', 'a', 'en', 'un', 'es', 'se', 'no', 'te', 'lo', 'le']
        french_words = ['le', 'la', 'de', 'et', 'à', 'un', 'il', 'que', 'ne', 'se', 'ce', 'pas', 'tout']
        
        text_lower = text.lower()
        
        english_count = sum(1 for word in english_words if word in text_lower)
        spanish_count = sum(1 for word in spanish_words if word in text_lower)
        french_count = sum(1 for word in french_words if word in text_lower)
        
        total_words = len(text.split())
        if total_words < 5:  # Too short to determine language
            return None, 0.0
        
        if english_count > spanish_count and english_count > french_count:
            confidence = min(0.9, english_count / total_words * 10)
            return 'en', confidence
        elif spanish_count > english_count and spanish_count > french_count:
            confidence = min(0.9, spanish_count / total_words * 10)
            return 'es', confidence
        elif french_count > english_count and french_count > spanish_count:
            confidence = min(0.9, french_count / total_words * 10)
            return 'fr', confidence
        
        return None, 0.0
    
    def _extract_conversation_text(self, conv: Dict[str, Any]) -> str:
        """Extract all text content from a conversation."""
        text_parts = []
        
        # Extract from conversation parts
        parts = conv.get('conversation_parts', {}).get('conversation_parts', [])
        for part in parts:
            if part.get('part_type') == 'comment':
                body = part.get('body', '')
                if body:
                    text_parts.append(body)
        
        # Extract from source
        source = conv.get('source', {})
        if source.get('body'):
            text_parts.append(source['body'])
        
        return ' '.join(text_parts)
    
    def _get_confidence_level(self, confidence: float) -> str:
        """Get confidence level string from numeric confidence."""
        if confidence >= self.high_confidence_threshold:
            return "high"
        elif confidence >= self.medium_confidence_threshold:
            return "medium"
        elif confidence >= self.low_confidence_threshold:
            return "low"
        else:
            return "none"
    
    def _clean_conversation_text(
        self, 
        conversations: List[Dict[str, Any]], 
        stats: Dict[str, Any]
    ) -> List[Dict[str, Any]]:
        """Clean and standardize text content in conversations."""
        cleaned_count = 0
        
        for conv in conversations:
            # Clean conversation parts
            parts = conv.get('conversation_parts', {}).get('conversation_parts', [])
            for part in parts:
                if part.get('body'):
                    original_body = part['body']
                    cleaned_body = self._clean_text(original_body)
                    if cleaned_body != original_body:
                        part['body'] = cleaned_body
                        part['body_cleaned'] = True
                        cleaned_count += 1
            
            # Clean source body
            source = conv.get('source', {})
            if source.get('body'):
                original_body = source['body']
                cleaned_body = self._clean_text(original_body)
                if cleaned_body != original_body:
                    source['body'] = cleaned_body
                    source['body_cleaned'] = True
                    cleaned_count += 1
        
        stats["text_cleaned_count"] = cleaned_count
        return conversations
    
    def _clean_text(self, text: str) -> str:
        """Clean and standardize text content."""
        if not text:
            return text
        
        # Remove HTML tags
        text = self.html_pattern.sub('', text)
        
        # Normalize whitespace
        text = re.sub(r'\s+', ' ', text)
        
        # Remove excessive punctuation
        text = re.sub(r'[!]{2,}', '!', text)
        text = re.sub(r'[?]{2,}', '?', text)
        
        # Normalize line breaks
        text = text.replace('\n', ' ').replace('\r', ' ')
        
        return text.strip()
    
    def _detect_outliers(
        self, 
        conversations: List[Dict[str, Any]], 
        stats: Dict[str, Any]
    ) -> List[Dict[str, Any]]:
        """Detect and flag outlier conversations."""
        if len(conversations) < 10:  # Need minimum data for outlier detection
            return conversations
        
        # Calculate text length statistics
        text_lengths = []
        for conv in conversations:
            text = self._extract_conversation_text(conv)
            text_lengths.append(len(text))
        
        if not text_lengths:
            return conversations
        
        # Simple outlier detection using IQR
        text_lengths.sort()
        q1 = text_lengths[len(text_lengths) // 4]
        q3 = text_lengths[3 * len(text_lengths) // 4]
        iqr = q3 - q1
        
        lower_bound = q1 - 1.5 * iqr
        upper_bound = q3 + 1.5 * iqr
        
        outliers_detected = 0
        for conv in conversations:
            text = self._extract_conversation_text(conv)
            text_length = len(text)
            
            if text_length < lower_bound or text_length > upper_bound:
                conv['is_outlier'] = True
                conv['outlier_reason'] = 'text_length'
                outliers_detected += 1
            else:
                conv['is_outlier'] = False
        
        stats["outliers_detected"] = outliers_detected
        return conversations
    
    def _statistical_sampling(
        self, 
        conversations: List[Dict[str, Any]], 
        max_count: int, 
        stats: Dict[str, Any]
    ) -> List[Dict[str, Any]]:
        """Perform statistical sampling to reduce dataset size."""
        if len(conversations) <= max_count:
            return conversations
        
        # Stratified sampling by category if available
        categories = {}
        for conv in conversations:
            category = conv.get('inferred_category', 'unknown')
            if category not in categories:
                categories[category] = []
            categories[category].append(conv)
        
        sampled = []
        samples_per_category = max_count // len(categories)
        
        for category, convs in categories.items():
            if len(convs) <= samples_per_category:
                sampled.extend(convs)
            else:
                # Random sampling within category
                import random
                sampled.extend(random.sample(convs, samples_per_category))
        
        # Fill remaining slots randomly
        remaining = max_count - len(sampled)
        if remaining > 0:
            remaining_convs = [conv for conv in conversations if conv not in sampled]
            if remaining_convs:
                import random
                sampled.extend(random.sample(remaining_convs, min(remaining, len(remaining_convs))))
        
        stats["sampling_applied"] = True
        stats["original_count"] = len(conversations)
        stats["sampled_count"] = len(sampled)
        
        return sampled
    
    def get_preprocessing_report(self, stats: Dict[str, Any]) -> str:
        """Generate a human-readable preprocessing report."""
        report = []
        report.append("=== Data Preprocessing Report ===")
        report.append(f"Original conversations: {stats['original_count']}")
        report.append(f"Processed conversations: {stats['processed_count']}")
        report.append(f"Deduplicated: {stats['deduplicated_count']}")
        
        if stats.get('inferred_data'):
            inferred = stats['inferred_data']
            report.append(f"Categories inferred: {inferred['categories_inferred']}")
            report.append(f"Topics inferred: {inferred['topics_inferred']}")
            report.append(f"Languages inferred: {inferred['languages_inferred']}")
            
            confidence_dist = inferred.get('confidence_distribution', {})
            report.append("Confidence distribution:")
            for level, count in confidence_dist.items():
                report.append(f"  {level}: {count}")
        
        if stats.get('validation_errors'):
            report.append(f"Validation errors: {len(stats['validation_errors'])}")
        
        if stats.get('outliers_detected'):
            report.append(f"Outliers detected: {stats['outliers_detected']}")
        
        if stats.get('sampling_applied'):
            report.append(f"Statistical sampling: {stats['sampled_count']} from {stats['original_count']}")
        
        return "\n".join(report)


# Custom Exceptions
class PreprocessingError(Exception):
    """Exception raised when preprocessing fails."""
    pass






